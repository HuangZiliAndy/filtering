


 Want Gabor-like filters that provide a self-adjoint frame... what a self-adjoint
 frame means in practice is that after getting the coefficients for each filter,
 you can just multiply the coefficients the filter functions directly
 with the conjugates of the filter functions and exactly reconstruct the signal.

 By "gabor-like" what I mean - and this may be totally the wrong terminology -
 is that each filter will be a real function with limited support times a complex
 exponential, i.e. the k'th filter function will be:

   f_k(t) = f(t) exp(- omega_k i t)

 where f(t) is the basic filter function.  (We numerically optimize f(t); it
 looks a little like the mexican hat function with a smaller-than-normal
 brim, and can be approximated to within 0.01 error by a sinc function times
 a Gaussian.)

 Let the input signal (the one that we want to apply this decomposition on)
 be a(t).

 We can sample the a_t *  f_k(t)  [* is convolution] at a lower sampling rate
 than a_t if f(t) has limited enough support in the frequency domain.

 The basic idea is to divide the signal into multiple streams, with each stream
 sampled at a lower sample rate and representing that signal band-limited to a
 particular part of the frequency spectrum.  To keep the filter functions
 time-local we can't have completely sharp cutoffs in the frequency domain, and
 we'll need to have filters that overlap somewhat in frequency to avoid major
 aliasing effects.  The basic idea will be intuitively quite similar to how mel
 filter-banks are normally constructed, with triangular filters; except that the
 actual computation is done in the time domain.

 For some background that may be relevant and to explain the terminology
 "self-adjoint frame", see the concept of a "frame" here:
   https://en.wikipedia.org/wiki/Frame_(linear_algebra)
 This frame will also be a "tight frame".

 Suppose we have a continuous band-limited input signal that's sampled at
 integer values of t (imagine it's at 1Hz; this is without loss of generality).
 It would be band-limited to the Nyquist frequency which is 0.5.  The angular
 frequency corresponding to the Nyquist is 2pi * 0.5 = pi; we'll be using
 angular frequencies as it simplifies some of the equations.  For now, assume
 the signal is complex; later on we'll handle the symmetries that rise if it's
 real-valued.

 The angular frequencies within this band range from -pi to pi; view them
 as going in a circle due to aliasing.  (The point of the circle is to stress
 that we don't have to treat the frequencies at pi/-pi specially just because
 they are at the edge of the range of allowed frequencies.)

 We are going to split up the signal into N separate streams where N is even;
 each stream will be sampled every N/2 seconds. (Remember: we assumed, without
 loss of generality, that the original signal was sampled at 1Hz).  Each stream
 with index k=0..N-1 will have a central angular-frequency of omega=(2pi (k +
 1/2) / N), i.e. it will span a complex-frequency band of the input that's
 centered at omega.  Note that the last bin would allow in some frequencies
 above the Nyquist, and the first bin would give us some negative frequencies,
 but this doesn't matter..  we can make the argument that if we had these bins
 all the way around the circle, including negative frequencies too, we'd get
 unit impulse response at all frequencies all the way around the circle.
 The above-Nyquist frequencies can just be interpreted as frequencies below
 the Nyqust on the negative frequency side.


 We are oversampling the signal by a factor of 2 (some oversampling is typically
 inevitable in these kinds of schemes, if you want good time/frequency locality);
 the factor of 2 oversampling is just an arbitrary choice, which we are
 making concrete for simplicity.  The width of each of the frequency bands, from
 top to bottom, is 2pi divided by (N/2) = 4pi/N, and the Nyquist of each of them
 would be pi * 2/N.  As a special case, if N == 2 this means that we split the
 signal up into two halves with the same sampling rate, i.e. the sampling rate
 doesn't change.


 The constraint we want to satisfy is that when we multiply by the shifted
 filter functions times the complex exponentials required to give us the
 center frequences of the N separate streams, we get back the original
 signal energy for any angular frequency in [-pi..pi].  This becomes a constraint
 on the frequency response of the filter functions.

 The way we actually optimize this numerically is that we choose the
 time-domain support of the filter function (e.g. -6..6 or -10..10),
 define a loss function to optmize, and train to convergence using gradient
 descent with momentum.

  For more details you can look at ./optimize_filter.py, but the objective
  function has 3 terms:

     - One penalizes nonzero gain for any frequency greater than pi
     - One ensures that the energy gains 'sum to one' in the appropriate
       way by ensuring that for 0 <= omega < pi/2, the frequency response
       F(.) satisfies F(omega)^2 + F(pi - omega) == 1.  That is, it
       penalizes nonzero values of (F(omega)^2 + F(pi - omega) - 1).
     - The third term penalizes high-frequency energy in the filter
       function; it's necessary because we only compute the frequency
       response F(omega) up to a very finite frequency, e.g. omega = 4 pi,
       so penalizing that doesn't penalize energy in frequencies higher
       than that.
       It is an l2 norm on a high-passed version of the filter function,
       where the cutoff of the high-pass filter is chosen to be the
       frequency that we stop computing F(omega) at, currently 4 pi.


============


  A note on which version of the Fourier transform we are using:
  we'll be using the angular-frequency version of the Fourier transform.

    F(omega)  =    \integral_{-infty}^{+infty} f(t) e^{-i \omega t } dt
    f(t)  =        \integral_{-\infty}^{+infty}  F(omega) e^{i t \omega}  d\omega

  We'll define in advance the extent of time support of f(t): say, -S to S where
  S might be, say, 8.  (We'll make S an integer for convenience).  We'll be optimizing
  f(t) numerically.

  f(t) and F(omega) will both be symmetric (because we want the phase
  unchanged), and of course real, so we can use a one-sided integral for
  F(omega), replace the complex exponential with a cosine, and multiply by 2:

    F(omega)  =   2   \integral_0^{+infty} f(t) cos(\omega t) dt     (eqn:1)

  We'll be evaluating F(omega) for omega = 0 ... T\pi, for another integer T
  that we define in advance (has to be large enough so that we can penalize
  high-frequency noise in f(t)).  The framework is that we put penalties on
  F(omega) to ensure the various properties that we want.

  CONSTRAINTS ON F:
    - F(omega)^2 + F(pi - omega)^2 = 1.
    - F(omega) = 0 for omega >= pi
  CONSTRAINT ON f:
    - Should have no energy above the frequency T pi.  We ensure this by
      penalizing a high-pass-filtered version of it.
    - Should be within 0.01 of a particular function defined in the code,
      which is a sinc function times a Gaussian.  This is just to avoid the
      optimization reaching bad local optima.  At convergence this constraint is
      never active.

   We'll be evaluating both f(t) and F(omega) at discrete points.  Suppose there
   are D equally spaced points per unit on the x-axis of f(t), and also D
   equally spaced points in each interval of size \pi in F(omega); there are T
   such intervals (e.g. T = 4; we need to make T high enough to prevent high
   frequency ringing, which would not be properly penalized if T was too small.

   We'll define f(t) at the points 0, 1/D, 2/D, ... (SD-1) / D; and
   we'll evalute F(omega) at the points 0, \pi/D, 2\pi/D, ... (TD - 1)\pi / D.

        t_d =  d / D    (for  0 <= d < S D)
      \omega_k = k \pi / D     (for 0 <= k < T D)

  Define (as a shorthand notation):
      f_d  ==  f(t_d) == f(d / D)
  .. the values f_d will be learnable parameters in our formulation, i.e. we are
  learning the vector of f_d from d = 0...SD.  Note: implicitly, f_{-d} == f_d,
  by symmetry, and for d >= SD, f_d would be zero.

  Also, define
     F_k = F(\omega_k) = F(k \pi / D),
  for 0 <= k < T D.
  We'll be evaluating this by approximating the integral as a sum over D
  separate regions.


  Referring back to (eqn:1), and using the notes above, we will approximate the integral
  expression that gives F(omega) as a sum.  We'll be evaluating this for the following points,
  for 0 <= k < T D:

    F_k = F(\omega_k) = F(k \pi / D); and also remember, t_d = d / D.

 So, for 0 <= k < T D:

   F_k = 2/D *   \sum_{d=0}^{SD-1} w_d  f_d cos(k d \pi / D^2)

 where w_d = 0.5 if d is 0 and 1.0 otherwise.

 f_d (for 0 <= d < SD) is the thing we're optimizing; F_k is the thing on which
 we want to impose various penalties and constraints (to get the frequency
 response we want); and all the other terms in there become the elements of a
 matrix.  The new factor 1/D is from the width of the discretized regions.

 Suppose the matrix is M with elements M_{kd}; we'd have:

    M_{kd} =  2/D *  w_d cos(k d \pi / D^2).

 We'll compute F = M f, where f is the vector of f_d.

 Search above for CONSTRAINTS ON F for the constraints we want to impose on F.
 It's mostly fairly obvious how we'd impose these.  The constraint:
   F(omega)^2 + F(pi - omega)^2 = 1
 amounts to ensuring that, for 0 < k <= D/2,
    F_k^2 + F_{D-k}^2 == 1 (approximately; we'll do both by putting a penalty on their
 l2 norm.

 To avoid high-frequency energy the signal f that is not appropriately penalized
 by the constraints on F_k (because we only evaluate it up to a finite
 frequency), we also penalize the l2 norm of a highpassed version of f, where the
 high-pass filter is chosen with a cutoff corresponding to the angular frequency
 of T pi (this is the highest frequency we explicitly penalize)... in regular
 non-angular frequency, this would be T/2 (divide by 2 pi).

 Also, to avoid the optimization reaching bad local optima, we force f_t to be
 close to a functional form that we found is a close approximation to the
 learned time-domain filter; this is a sinc function times a Gaussian (see the
 code for details), and it's correct to over 99% in l1 integral with maximum
 absolute deviation less than 0.01.

==================

 NOTES ON CONSTANT FACTORS.

 To compute the band-limited data, we can simply convolve the input signal with
 the impulse reponse f(t) that we'll obtain by the procedure described above.

 We're assuming the sampling rate of the input data was 1Hz.
 Suppose the sampling rate of the band-limited data was 1/N Hz.
 In order to reconstruct the original 1Hz data from this data we'd have to
 apply a factor of N (it's fairly obvious why, as there are a factor of N
 fewer points to sum over).

 The above is only valid, though, if we have subsampled streams all the way
 around the unit circle, i.e. including negative frequencies as well as
 positive.  Because, to avoid redundancy, we only compute the filters for
 positive frequencies we need an extra factor of 2 for all filters that have a
 twin that has been omitted (so: 2N, instead of N.)  (The real part of the
 positive- and negative-frequency constributions would be the same, and the
 imaginary part should cancel, so when reconstructing we just take the real part
 of the positive-frequency component and double it.










 Want Gabor-like filters that provide a self-adjoint frame... what a self-adjoint
 frame means in practice is that after getting the coefficients for each filter,
 you can just multiply the coefficients the filter functions directly
 with the conjugates of the filter functions and exactly reconstruct the signal.

 By "gabor-like" what I mean - and this may be totally the wrong terminology -
 is that each filter will be a real function with limited support times a complex
 exponential.  The basic idea is to divide the signal into multiple streams,
 with each stream sampled at a lower sample rate and representing that signal
 band-limited to a particular part of the frequency spectrum.  To keep the
 filter functions time-local we can't have sharp cutoffs in the frequency domain,
 and we'll need to have filters that overlap somewhat in frequency.  The basic
 idea will be intuitively quite similar to how mel filter-banks are normally
 constructed, with triangular filters; except that the actual computation is
 done in the time domain.

 For some background that may be relevant and to explain the terminology
 "self-adjoint frame", see the concept of a "frame" here:
   https://en.wikipedia.org/wiki/Frame_(linear_algebra)
 This frame will also be a "tight frame".

 Suppose we have a continuous band-limited input signal that's sampled at
 integer values of t (imagine it's at 1Hz; this is without loss of generality).
 It would be band-limited to the Nyquist frequency which is 0.5.  The angular
 frequency corresponding to the Nyquist is 2pi * 0.5 = pi; we'll be using
 angular frequencies as it simplifies some of the equations.  For now, assume
 the signal is complex; later on we'll handle the symmetries that rise if it's
 real-valued.

 The angular frequencies within this band limit range from -pi to pi; view them
 as going in a circle due to aliasing.  (The point of the circle is to stress
 that we don't have to treat the frequencies at pi/-pi specially just because
 they are at the edge of the range of allowed frequencies.)

 We are going to split up the signal into N separate streams where N is even;
 each stream will be sampled every N/2 seconds. (Remember: we assumed, without loss
 of generality, that the original signal was sampled at 1Hz).  Each stream with
 index k=0..N-1 will have a central angular-frequency of omega=(2pi k / N),
 i.e. it will span a complex-frequency band of the input that's centered at
 omega.  Note that this gives us frequencies above the Nyquist, but we are
 viewing it as circular; this just makes the indexing slightly easier than
 having k range from -N/2 to N/2-1.

 We are oversampling the signal by a factor of 2 (some oversampling is typically
 inevitable in these kinds of schemes, if you want good time/frequency locality);
 the factor of 2 oversampling is just an arbitrary choice, which we are
 making concrete for simplicity.  The width of each of the frequency bands, from
 top to bottom, is 2pi divided by (N/2) = 4pi/N, and the Nyquist of each of them
 would be 2pi/N.  As a special case, if N == 2 this means that we split the
 signal up into two halves with the same sampling rate, i.e. the sampling rate
 doesn't change.

 For the frequency band with k=0 we'll essentially be convolving the time-domain
 signal with a symmetric filter function with limited support in.  We want this
 filter function to have limited support in the frequency domain as well.
 (Technically doing both of these as the same time is impossible, but we can get
 close.)

 The constraint we want to satisfy is that when we multiply by the shifted
 filter functions times the complex exponentials required to give us the
 center frequences of the N separate streams, we get back the original
 signal energy for any angular frequency in [-pi..pi].  This becomes a constraint
 on the frequency response of the filter functions.

 We'll define the canonical filter as a function F(omega) with support on
 -pi..pi.  (We'll scale down on the frequency axis by N/2 to get the actual
 filters).  We'll analyze the response of this system in terms of pure
 sinusoids (it's all linear and the sinusoids are a complete basis, so this
 is sufficient).  Being a little bit approximate about constant factors:
 when we convolve the filter with that sinusoid at frequency omega, we'll
 get a signal with amplitude F(omega) and of course the same frequency and
 phase; when we reconstruct using the same filter functions, we again
 incur the factor of F(omega), so the roundtrip gain is F(omega)^2.
 Each frequency, in general, will have nonzero response from two of these
 filters because they overlap.  To get unit gain, we need the F(omega)^2
 values to sum up to 1 when appropriately shifted in frequency space.
 Made concrete, and remembering that F(omega) is defined on [-pi..pi] and
 is symmetric: this is equivalent to saying,
    for 0 <= omega <= pi/2,

       F(omega)^2 + F(pi - omega)^2 = 1.

 This would be satisfied, for instance, by defining  F(omega) = sqrt(1 - omega/pi).
 We need more than that, though: we need F(omega) to be such that when we
 take the inverse Fourier transform of F(.) and get the time-domain filter f(t),
 that function f(t) has very small values outside a certain specified region (say,
 -4 <= t <= 4).  That will allow us to compute it efficiently.

 (possibly some relevant background here:
  http://www.its.caltech.edu/~matilde/GaborLocalization.pdf)


============


  The canonical frequency response will be a symmetric function F(omega),
  defined on -pi <= omega <= pi.  We will F(pi) = 0 to avoid problems
  with phase at the Nyquist; this implies F(0) = 1.

  As mentioned above, to avoid any variation in response of the
  system at different frequencies we will require,  for 0 <= omega <= pi/2,

      F(omega)^2 + F(pi - omega)^2 = 1.

  We'll also require F(omega) >= 0, to avoid any phase flipping which
  might lead to strange effects.   Note: the above implies that F(pi/2) = sqrt(2).
  We will numerically optimize F(omega) to minimize the function values outside
  a specified time window.  Because of the symmetries above, we only need to
  optimize F(omega) for 0 < omega < pi/2; these values define the remaining values.


  We want the Fourier transform of this function to have as tight support as
  possible.
  Let F(y) be the fourier transform of x (physically evaluated from, say,
  -16 to +16).

  We'll require it to be bounded by a Gaussian with as small
  a variance as possible.  (Note: this probably only makes sense when evaluated
  on a finite time interval, since F(x) has bounded support and I'm not sure that
  it's possible for f(x) to drop off that fast in the limit.  But this is fine
 for our purposes.)

  Let the inverse variance be S, which we'll want
  to make as large as possible.  We know that f(0) will be less than 2 pi,
  likely less than pi.

    L(y, S) = max(0,  log( f(y) / (2 pi exp( - 0.5 S y^2 )) ))

  If f(y) is bounded by the function  exp( - 0.5 S y^2 ), L(y, S) will be zero
       for all y.
  Suppose we are evaluating f(y) at K distinct points y_k for k=0..K-1.

  The loss function will be:

      O = -S + (1/K) sum_k L(y_k, S).


=========

 For the physical computation we'll discretize it.  The function f(x), defined for x=0...pi,
 will be split up into J equal-sized intervals, of size pi/J.  (Say, J=128 or something like that).

 It is the points at  F( pi j / J) for 0 < j < J which we need to define; there are J-2 of
 them.  ( F(0) and F(pi) are 1 and 1/sqrt(2) respectively. )  Let us write
 F_j as a shorthand for F(pi j / J)

 Next we consider the inverse fourier transform of F(omega): f(t).  Note: we are
 using the angular-frequency versions of these equations, which are a little
 different.  Canonically this would be:

     f(t) = 1/(2pi)  \integral_{omega=-infinity}^{infinity} exp(i omega t) F(omega)

 and because F(omega) is real and symmetric, we can write this using cosines only,
 and integrating over [0..pi]:

     f(t) = 1/pi  \integral_{omega=0}^{pi} cos(omega t) F(omega)

 We'll define F(omega) for the points

     omega_j = pi j / J,
 and define:
     F_j   =   F(omega_j)
 (we'll explain below specifically how we compute F_j).

 We can approximate the integral as a sum; note, the size of the segmentas is
 pi/J, so we multiply by that.   Define c_j as 1 if j>0, and 1/2 if
 j is 0 (because in that cases we only use half the segment).

 We have:
    f(t) =   1/J  \sum_{j=1}^2J  c_j cos(omega_j t) F_j

    f(t) =   1/J  \sum_{j=1}^2J  c_j cos(pi j t / J) F_j                 (eqn:1)


 Because F(omega) only has support for 0 <= omega < 2pi, we can limit the
 integral  that
 range.  Note:  we only directly define F(omega) for 0 < omega  < pi;
 for omega > pi, F(omega) = sqrt(1 - F(2pi - omega)^2).

 Define omega_j as:

    omega_j = pi j / J,

 We define quantities \theta_j for j = 1..J-1; these are the trainable
 parameters which will define the values of F(omega_j).

  We'll define F_j as follows:

    F_j = abs(cos(\theta_j))  for 0 < j < J, and
    F_j = abs(sin(\theta_{2J - j})) for J < j < 2J.
    F_0 = 1
    F_J = sqrt(2)

 Let there be J intervals per unit time on the time axis.  We evalute the function
 of time on a finite interval, say -4..4; but f(t) is also symmetric, so we can
 focus on 0 <= t < 4.

 Let's use the index k for the index on the time axis.  We'll evaluate f(t_j)
   for 0 <= k < 4J, defining:
  t_k = k / J.

 Define f_k = f(t_k).  Substituting t_k = k / J, we have:

    f_k =  1/J  \sum_{j=1}^2J  c_j cos(pi j k / J^2) F_j                (eqn:2)


 We can compute a matrix of the f_k from the F_j quantities, it becomes a matrix
 multiplication with the cosines (and c_j) as the elements of the matrix.

 Let this matrix be M_{k,j}.  Its entries corresponds to the coefficients in (eqn:2),
 so:

 So it's:
    M_{k,j} = 1/J * (j == 0 ? 0.5 : 1) *  cos(pi * j * k / J^2).

 Objective function:

  To start with, could minimize sum-of-squares of the higher-numbered f_k;
  say those with 2J <= k < 4J.



